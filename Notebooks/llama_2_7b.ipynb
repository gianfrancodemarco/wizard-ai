{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJ_p-IqabZAC"
      },
      "source": [
        "## Transformers library + Flask + Ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncUXmSr_bl3J",
        "outputId": "7df2c33e-e367-4cd7-a778-936cde2a624a"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login --token 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\n",
        "!pip install flask\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_XilZN6blSx"
      },
      "outputs": [],
      "source": [
        "import threading\n",
        "\n",
        "import torch\n",
        "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load the LLAMA7b model and tokenizer\n",
        "token = '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'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Configuration parameters\n",
        "params = {\n",
        "    \"low_cpu_mem_usage\": True,\n",
        "    \"torch_dtype\": torch.float16,\n",
        "}\n",
        "\n",
        "# Load the model configuration\n",
        "config = AutoConfig.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", stream=True)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
        "\n",
        "# Load the model\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", config=config, **params)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from flask import Flask, jsonify, request\n",
        "from pyngrok import ngrok\n",
        "from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "\n",
        "# Stop current ngrok tunnel\n",
        "for tunnel in ngrok.get_tunnels():\n",
        "  ngrok.disconnect(tunnel.public_url)\n",
        "\n",
        "\n",
        "class KeywordsStoppingCriteria(StoppingCriteria):\n",
        "    def __init__(self, keywords_ids:list):\n",
        "        self.keywords = keywords_ids\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        if input_ids[0][-1] in self.keywords:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "\n",
        "DEFAULT_GENERATION_CONFIGS = {\n",
        "    \"max_new_tokens\": 1024,\n",
        "    \"top_k\": 1 # Reproducibility\n",
        "}\n",
        "\n",
        "app = Flask(__name__)\n",
        "@app.route('/v1/completition', methods=['POST'])\n",
        "def completition():\n",
        "    try:\n",
        "        # Get the input data from the request\n",
        "        data = request.json\n",
        "        prompt = data['prompt']\n",
        "        configs = {\n",
        "            **DEFAULT_GENERATION_CONFIGS,\n",
        "            **data.get('configs', {})\n",
        "        }\n",
        "\n",
        "        # Configure stopping criteria\n",
        "        stopping_criterias = []\n",
        "        if \"stop\" in configs:\n",
        "          stop_words = configs[\"stop\"] or []\n",
        "          stop_ids = [tokenizer.encode(w)[0] for w in stop_words]\n",
        "          stopping_criterias = StoppingCriteriaList([KeywordsStoppingCriteria(stop_ids)])\n",
        "          del configs[\"stop\"]\n",
        "\n",
        "        # Tokenize the prompt\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "        inputs.to(device)\n",
        "\n",
        "        # Generate text\n",
        "        generate_ids = model.generate(\n",
        "            inputs.input_ids,\n",
        "            stopping_criteria=stopping_criterias,\n",
        "            **configs\n",
        "        )\n",
        "        response = tokenizer.batch_decode(\n",
        "            generate_ids,\n",
        "            skip_special_tokens=True,\n",
        "            clean_up_tokenization_spaces=False\n",
        "        )[0]\n",
        "\n",
        "        return jsonify({'completition': response})\n",
        "    except Exception as e:\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "# Open a ngrok tunnel to the HTTP server\n",
        "port = 8080\n",
        "public_url = ngrok.connect(port).public_url\n",
        "print(\" * ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:{}\\\"\".format(public_url, port))\n",
        "# Update any base URLs to use the public ngrok URL\n",
        "app.config[\"BASE_URL\"] = public_url\n",
        "# Start the Flask server in a new thread\n",
        "threading.Thread(target=app.run, kwargs={\"use_reloader\": False, \"port\":port}).start()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "2YZ7UY08Kk6O",
        "O4baGxJJKqhu"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "VEnv",
      "language": "python",
      "name": "venv"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
