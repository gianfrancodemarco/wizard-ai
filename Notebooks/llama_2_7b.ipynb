{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJ_p-IqabZAC"
      },
      "source": [
        "## Transformers library + Flask + Ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncUXmSr_bl3J"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login --token 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\n",
        "!pip install flask\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_XilZN6blSx"
      },
      "outputs": [],
      "source": [
        "import threading\n",
        "\n",
        "import torch\n",
        "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load the LLAMA7b model and tokenizer\n",
        "token = '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'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Configuration parameters\n",
        "params = {\n",
        "    \"low_cpu_mem_usage\": True,\n",
        "    \"torch_dtype\": torch.float16,\n",
        "}\n",
        "\n",
        "# Load the model configuration\n",
        "config = AutoConfig.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", stream=True)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
        "\n",
        "# Load the model\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", config=config, **params)\n",
        "model.eval()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ci1Gn5lPnVN3",
        "outputId": "4f3db7bc-a172-4152-b09c-0f837a2d75fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-12-16T00:31:56+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8084-5e36bf97-79a0-4210-8840-58a89bf8073f acceptErr=\"failed to accept connection: Listener closed\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * ngrok tunnel \"https://e440-34-147-75-23.ngrok.io\" -> \"http://127.0.0.1:8086\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:8086\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from flask import Flask, jsonify, request\n",
        "from pyngrok import ngrok\n",
        "from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "\n",
        "# Stop current ngrok tunnel\n",
        "for tunnel in ngrok.get_tunnels():\n",
        "  ngrok.disconnect(tunnel.public_url)\n",
        "\n",
        "\n",
        "class KeywordsStoppingCriteria(StoppingCriteria):\n",
        "    def __init__(self, keywords_ids:list):\n",
        "        self.keywords = keywords_ids\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        if input_ids[0][-1] in self.keywords:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "\n",
        "DEFAULT_GENERATION_CONFIGS = {\n",
        "    \"max_new_tokens\": 1024,\n",
        "    \"top_k\": 1 # Reproducibility\n",
        "}\n",
        "\n",
        "app = Flask(__name__)\n",
        "@app.route('/v1/completition', methods=['POST'])\n",
        "def completition():\n",
        "    try:\n",
        "        # Get the input data from the request\n",
        "        data = request.json\n",
        "        prompt = data['prompt']\n",
        "        configs = {\n",
        "            **DEFAULT_GENERATION_CONFIGS,\n",
        "            **data.get('configs', {})\n",
        "        }\n",
        "\n",
        "        # Configure stopping criteria\n",
        "        stopping_criterias = []\n",
        "        if \"stop\" in configs:\n",
        "          stop_words = configs[\"stop\"] or []\n",
        "          stop_ids = [tokenizer.encode(w)[0] for w in stop_words]\n",
        "          stopping_criterias = StoppingCriteriaList([KeywordsStoppingCriteria(stop_ids)])\n",
        "          del configs[\"stop\"]\n",
        "\n",
        "        with torch.no_grad():\n",
        "          # Tokenize the prompt\n",
        "          inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "          inputs.to(device)\n",
        "\n",
        "          # Generate text\n",
        "          generate_ids = model.generate(\n",
        "              inputs.input_ids,\n",
        "              stopping_criteria=stopping_criterias,\n",
        "              **configs\n",
        "          )\n",
        "          response = tokenizer.batch_decode(\n",
        "              generate_ids[:, inputs.input_ids.shape[1]:], # Decode only the generated part\n",
        "              skip_special_tokens=True,\n",
        "              clean_up_tokenization_spaces=False\n",
        "          )[0]\n",
        "\n",
        "          return jsonify({\n",
        "              'completition': response,\n",
        "              'input_tokens': len(inputs.input_ids),\n",
        "              'output_tokens': len(generate_ids[0]),\n",
        "              'total_tokens': len(inputs.input_ids) + len(generate_ids[0])\n",
        "            })\n",
        "    except Exception as e:\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "# Open a ngrok tunnel to the HTTP server\n",
        "port = 8086\n",
        "public_url = ngrok.connect(port).public_url\n",
        "print(\" * ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:{}\\\"\".format(public_url, port))\n",
        "# Update any base URLs to use the public ngrok URL\n",
        "app.config[\"BASE_URL\"] = public_url\n",
        "# Start the Flask server in a new thread\n",
        "threading.Thread(target=app.run, kwargs={\"use_reloader\": False, \"port\":port}).start()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FWwbQ_7SDIua"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "VEnv",
      "language": "python",
      "name": "venv"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}